Quantification



Although numbers seem today to be entirely natural and quantification an elementary process, that is only because so many tools, techniques, institutions, and infrastructures have developed over millennia---and especially over the last few hundred years---to churn out numbers and make the world intelligible through them.

Quantification is a fundamental cultural process that depends on and drives other such processes. It requires grouping things together and judging them similar enough to be counted. This we call categorization or classification. It works best when all counters use the same means of measuring and the same sized counting units. This we call standardization. It requires managing teams of observers ranging in size, from the lone bean counter to an army of surveyors. The numbers that those observers gathered may then be abstracted further from their particular origins by many and various mathematical manipulations, from simple sums to regression analyses.

The earliest examples of quantification date back to near the dawn of agriculture and well before the advent of written history---indeed some scholars argue that quantification acted as a stepping stone toward all writing. In the beginning, categories were narrow and each category came with its own means of counting. Looking at the ancient Near East between 7500 and 3500 BC, Denise Schmandt-Besserat explains that every good had to be represented by a particular token. One counted grain with grain tokens and garments with garment tokens. With the rise of cities came clay tablets bearing marks for counting and keeping track of debts. On those tablets, the symbols for quantities of grain---themselves already standardized---broke free of their specificity and became instead abstracted numerals fit to be used in counting within any or all classifications.[Khipu too, or cross-ref at least]

In the modern era quantification is inescapable and often industrial in character. One of the largest life insurance companies in the world in the early twentieth century developed a technique for assessing the risk of individual lives and called it the "numerical method." In publicity describing the company's mechanism for rating its hundreds of thousands of applicants, it employed a series of photographs to illustrate its "human treadmill." A picture taken of letters streaming into the mailroom came early in the series. Such letters bore standardized forms filled in by applicants, agents, or examining physicians--each already abounding in numbers from earlier quantifying processes. Those forms eventually made their way to the white women in white blouses and white men in vests, jackets, and ties sitting in rows upon rows of the office tower in a New York City skyscraper. Those assembled workers disassembled each application into its key questions and determined how to classify each answer to each question, assigning a point value (often derived from consulting a table of printed numbers) to the resulting classifications. Then the worker added and subtracted these classifications-cum-ratings according to a fixed algorithm. The resulting number stood in for the insurance applicant and in most cases justified, on its own, the company's decision to accept, reject, or impose a penalty on the applicant. What is striking in this picture is not only the vast scale of the quantifying operation, but also how many numbers had to be made and how many others consulted in the process. A person---or a commodity, or a physical phenomena---became a number, but only after becoming a number many times before.

The processes that filled the modern world with numbers depended on wide-spread observation mediated by specialized instruments and new techniques for managing and presenting numerical data. When the naturalist Alexander von Humboldt traveled to South America determined to reveal the laws linking life to its surroundings he carried with him an observatory's worth of thermometers, barometers, [xxxx], borne on the backs of mules guided by indigenous workers. To make sense of all his measurements, Humboldt turned to maps on which he could plot his figures and draw "isolines" connecting them. The widespread production of graph paper in the early nineteenth century facilitated further reliance on observational data and the use of graphical techniques to make sense of proliferating observations. Astronomers and actuaries alike made curves drawn by hand through plotted numbers important tools for both business and science.

Ian Hacking explained the early nineteenth century "avalanche of printed numbers" in Europe by tracing those numbers to Victorian bureaucracies responsible for improving the lives of the poor and potentially revolutionary workers. With more numbers came the possibility to look for and discover patterns or regularities, many of which became new objects of study in themselves. Hacking's key example was sickness, a state of being that came to be seen as explicable by statistical laws in the context of the avalanche and, once in the grasp of standardizing bureaucrats attending international statistical congresses, produced a host of new and newly defined kinds of illness. Indeed today, a person diagnosed with high blood pressure enters medical treatments, even though their condition is in and of itself only an indicator or high risk for some future bodily ailment.

Corporate offices set off their own, later, avalanches of printed numbers. Railroads and telephone companies built vast networks for coordinating the flows of both goods and information. They---and the mass marketers who grew along with them---sent agents to far-flung peripheries, built branches, and advertised widely. Quantification grew as a means for disciplining employees. Home offices that did not trust their field divisions to keep the corporation's best interests at heart forced them to fill in complicated blank forms, many of which only accepted numbers. Relying more on figures from account books or objective measures from precision scales also meant relying less on the judgment of ill-trained agents. Of course, those who provided the figures could till doctor them, or game them to win some advantage. States responded to corporate expansion with their own systems for measuring the honesty and probity of big business and eventually of government itself, with elaborate twentieth century budgeting programs. Those efforts linger, especially in the form of cost-benefit analyses. Quantification solved many problems associated with growth, but created new ones. It encouraged governance based on numbers that, even if faithfully rendered, tended to privilege whatever was most readily quantified rather than what experts or bureaucrats judged to be most significant.

The trend of twentieth century has been toward bigger and also nimbler networks of observers and instruments supported first by empires, then new international governance bodies (the League of Nations and the United Nations,) and later by Cold War alliances. They invented new globe-spanning numbers, sometimes unintentionally, that reframed debates about international politics as engineering problems. With the calorie serving as a standard unit, national and global indices of hunger could be created and used to justify imperial or international intervention or justify the distribution of subsidized food aid. World population figures would eventually feed major efforts to curtail or control fertility, efforts led by nation states with funding from major philanthropic foundations. But determining such a figure after WWII required first reconciling national censuses of varying quality, coverage, and organization. Similar challenges confronted efforts to represent global climate---or assign and then track annual average global temperatures. To meet those challenges, researchers across fields turned to computer-powered data models, integrating them into the quantifying treadmill. Now, in the early twenty-first century, much quantification results from people and things who, via the internet, act as constant observers of themselves and their surroundings. Some of the numbers they produce remain private, some combine to form risk profiles in various national security databases, and many are stored by a new breed of corporations for whom such data acts like capital.

Over the last few hundred years, quantification has been closely associated with objectivity. Such associations have proven particularly useful over the last century to bureaucrats and scientists alike who sacrificed a modicum of professional judgment in favor of allowing numerical criteria---from credit scores to p-values---the power to make more defensible, because objective, decisions for them. Focusing on the way quantification constrains subjectivity, however, distracts from one of the most important explanations for quantification and its effects. A complex range of emotions drive people to make numbers, as recent scholarship has emphasized. For example, Nathaniel Bowditch embraced the task of calculating numerical tables that sailors used to find their longitude at sea. He ferreted out errors in others' tables and made his extraordinarily precise. While others considered such work drudgery, Bowditch approached his long hours as a form of aesthetic or spiritual pursuit, according to Tamara Plakins Thornton. Numerical tables could also communicate emotions and aesthetics to readers, like those described by Jacqueline Wernimont who found a glimpse of the sublime and hints of human mastery even in John Graunt's tallies of those who died from plague in 17th century Britain. Or, as Caitlin Zaloom discovered, commodity traders at the end of the twentieth century lived in a world awash with market numbers which most learned to interpret less as individual facts than as a mass whose motions must be watched and judged, all as part of getting a feel for the market. On a larger scale, numbers like those produced by pollsters and marketers often shaped how ordinary people, like those discussed by Sarah Igo, understood themselves, their communities, or their nation. Given the range of emotions that flow through the process of quantification and the expertise and judgment so evidently necessary to successfully quantify, what is remarkable about the association of numbers with simple objectivity is that it has been so widely believed. [or reframe last sentence to say that belief in objectivity is one factor that has limited our fuller understanding of quantification as a historically situated process]

[something about categories absorbing some and excluding others?]
